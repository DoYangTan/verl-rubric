import time
import requests
import os
import json
from datetime import datetime

# ================= é…ç½®åŒºåŸŸ =================
# ä½ çš„ vLLM æœåŠ¡åœ°å€
BASE_URL = "http://localhost:8001"
# å¼ºåˆ¶è®¾ç½®ä¸èµ°ä»£ç†ï¼ˆè¿™æ˜¯è¿é€šçš„å…³é”®ï¼ï¼‰
os.environ["no_proxy"] = "localhost,127.0.0.1,0.0.0.0"
# ===========================================

def get_colored(text, color):
    colors = {"green": "\033[92m", "red": "\033[91m", "yellow": "\033[93m", "cyan": "\033[96m", "reset": "\033[0m"}
    return f"{colors.get(color, '')}{text}{colors['reset']}"

def get_vllm_metrics():
    """è·å– vLLM å†…éƒ¨è´Ÿè½½çŠ¶æ€"""
    try:
        # vLLM é»˜è®¤ä¼šåœ¨ /metrics æš´éœ² Prometheus æ ¼å¼æŒ‡æ ‡
        resp = requests.get(f"{BASE_URL}/metrics", timeout=1)
        if resp.status_code != 200:
            return None
        
        metrics = {}
        for line in resp.text.split('\n'):
            if line.startswith("vllm:num_requests_running"):
                metrics['running'] = float(line.split()[-1])
            elif line.startswith("vllm:num_requests_waiting"):
                metrics['waiting'] = float(line.split()[-1])
        return metrics
    except:
        return None

def test_inference(model_name):
    """å‘é€çœŸå®è¯·æ±‚æµ‹è¯•æ¨¡å‹ååº”"""
    url = f"{BASE_URL}/v1/chat/completions"
    payload = {
        "model": model_name,
        "messages": [{"role": "user", "content": "Hi"}],
        # å»ºè®®è°ƒå¤§ max_tokensï¼Œ5å¤ªå°äº†å®¹æ˜“å¯¼è‡´æˆªæ–­æˆ–æ— å†…å®¹
        "max_tokens": 50, 
        "temperature": 0.1
    }
    try:
        start = time.time()
        resp = requests.post(url, json=payload, timeout=10) # ç¨å¾®å¢åŠ ä¸€ç‚¹è¶…æ—¶æ—¶é—´
        latency = (time.time() - start) * 1000
        
        if resp.status_code == 200:
            data = resp.json()
            # === ä¿®æ”¹å¼€å§‹ï¼šå®‰å…¨è·å– content ===
            try:
                message = data['choices'][0]['message']
                content = message.get('content')
                
                # å¦‚æœ content æ˜¯ None (ä¾‹å¦‚åªè¾“å‡ºäº† reasoning_content æˆ–çº¯å·¥å…·è°ƒç”¨)
                if content is None:
                    # å°è¯•è·å–æ¨ç†å†…å®¹ (é’ˆå¯¹ R1/æ¨ç†ç±»æ¨¡å‹)
                    reasoning = message.get('reasoning_content', '')
                    if reasoning:
                        res_text = f"[æ­£åœ¨æ€è€ƒ] {reasoning[:20]}..."
                    else:
                        res_text = "<è¿”å›å†…å®¹ä¸ºç©º>"
                else:
                    res_text = content.strip()
                    if not res_text:
                        res_text = "<è¿”å›ç©ºç™½å­—ç¬¦>"
            except Exception as parse_err:
                 return True, f"è§£æå¼‚å¸¸: {str(parse_err)} ({latency:.1f}ms)"
            # === ä¿®æ”¹ç»“æŸ ===

            return True, f"{res_text} ({latency:.1f}ms)"
        else:
            return False, f"HTTP Error {resp.status_code}"
    except Exception as e:
        return False, str(e)
    """å‘é€çœŸå®è¯·æ±‚æµ‹è¯•æ¨¡å‹ååº”"""
    url = f"{BASE_URL}/v1/chat/completions"
    payload = {
        "model": model_name,
        "messages": [{"role": "user", "content": "Hi"}],
        "max_tokens": 5,
        "temperature": 0.1
    }
    try:
        start = time.time()
        resp = requests.post(url, json=payload, timeout=5)
        latency = (time.time() - start) * 1000
        
        if resp.status_code == 200:
            res_text = resp.json()['choices'][0]['message']['content'].strip()
            return True, f"{res_text} ({latency:.1f}ms)"
        else:
            return False, f"HTTP Error {resp.status_code}"
    except Exception as e:
        return False, str(e)

def get_model_name():
    """è‡ªåŠ¨è·å–æ¨¡å‹åç§°"""
    try:
        resp = requests.get(f"{BASE_URL}/v1/models", timeout=2)
        return resp.json()['data'][0]['id']
    except:
        return "model_weight/openai/gpt-oss-120b" # å…œåº•

def main():
    print(get_colored(f"ğŸš€ å¼€å§‹ç›‘æ§ vLLM ç«¯å£: {BASE_URL}", "cyan"))
    print(f"ä»£ç†çŠ¶æ€: no_proxy={os.environ.get('no_proxy')}")
    print("-" * 60)
    
    # 1. å…ˆè·å–ä¸€æ¬¡æ¨¡å‹å
    model_name = get_model_name()
    print(f"ç›®æ ‡æ¨¡å‹: {model_name}\n")

    counter = 0
    while True:
        timestamp = datetime.now().strftime("%H:%M:%S")
        
        # 2. è·å–è´Ÿè½½æŒ‡æ ‡ (è¿™æ˜¯æœ€é‡è¦çš„ç›‘æ§ï¼)
        metrics = get_vllm_metrics()
        
        if metrics is None:
            status = get_colored("è¿æ¥å¤±è´¥ (Offline)", "red")
            detail = "è¯·æ£€æŸ¥ 1.ç«¯å£æ˜¯å¦å¼€å¯ 2.é˜²ç«å¢™"
        else:
            # æ ¹æ®è´Ÿè½½å˜è‰²
            r_color = "green" if metrics.get('running', 0) > 0 else "yellow"
            status = get_colored("æœåŠ¡åœ¨çº¿ (Online)", "green")
            detail = f"æ­£åœ¨å¤„ç†: {get_colored(int(metrics.get('running', 0)), r_color)} | æ’é˜Ÿä¸­: {int(metrics.get('waiting', 0))}"

        print(f"[{timestamp}] {status} | {detail}", end="\r")

        # 3. æ¯ 5 ç§’å‘ä¸€æ¬¡æµ‹è¯•è¯·æ±‚ï¼Œè¯æ˜å®ƒèƒ½è¯´è¯
        if counter % 5 == 0 and metrics is not None:
            print() # æ¢è¡Œé˜²æ­¢è¦†ç›–
            success, msg = test_inference(model_name)
            if success:
                print(f"   â””â”€â”€ [æµ‹è¯•ç”Ÿæˆ] æˆåŠŸ: {get_colored(msg, 'cyan')}")
            else:
                print(f"   â””â”€â”€ [æµ‹è¯•ç”Ÿæˆ] å¤±è´¥: {get_colored(msg, 'red')}")
            print("-" * 60)
        
        counter += 1
        time.sleep(1)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nç›‘æ§ç»“æŸ")